{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: Programando LSTM with Keras and TensorFlow\n",
    "\n",
    "Las redes neuronales multicapas solo tienen conexiones directas. Las redes neuronales de este tipo siempre comienzan con una capa de entrada conectada a la primera capa oculta. Cada capa oculta siempre se conecta a la siguiente capa oculta. La última capa oculta siempre se conecta a la capa de salida. Esta forma de conectar capas es la razón por la que estas redes se denominan \"feedforward\". Las redes neuronales recurrentes no son tan rígidas, ya que también se permiten conexiones hacia atrás. Una conexión recurrente vincula una neurona en una capa con una capa anterior o con la neurona misma. La mayoría de las arquitecturas de redes neuronales recurrentes mantienen el estado de las conexiones recurrentes. Las redes neuronales feedforward no mantienen ningún estado. El estado de una red neuronal recurrente actúa como una especie de memoria a corto plazo para la red neuronal. En consecuencia, una red neuronal recurrente no siempre producirá la misma salida para una entrada determinada.\n",
    "\n",
    "Las redes neuronales recurrentes solamente se limitan pasar información de una capa a la siguient. Una conexión recurrente ocurre cuando se forma una conexión entre una neurona y uno de los siguientes tipos de neuronas:\n",
    "\n",
    "* La neurona misma\n",
    "* Una neurona al mismo nivel\n",
    "* Una neurona en un nivel anterior\n",
    "\n",
    "Las conexiones recurrentes nunca pueden apuntar a las neuronas de entrada ni a las neuronas de bias.\n",
    "El procesamiento de conexiones recurrentes puede resultar complicado. Debido a que los enlaces recurrentes crean bucles infinitos, la red neuronal debe tener alguna forma de saber cuándo detenerse. Una red neuronal que entrara en un bucle sin fin no sería útil. Para evitar bucles sin fin, podemos calcular las conexiones recurrentes con los siguientes tres enfoques:\n",
    "\n",
    "* Neuronas de contexto\n",
    "* Cálculo de la salida sobre un número fijo de iteraciones\n",
    "* Calculando la salida hasta que la salida de la neurona se estabilice\n",
    "\n",
    "La neurona de contexto es un tipo de neurona especial que recuerda su entrada y proporciona esa entrada como salida la próxima vez que calculemos la red. Por ejemplo, si le damos a una neurona de contexto 0.5 como entrada, generará 0. Las neuronas de contexto siempre generan 0 en su primera llamada. Sin embargo, si le damos a la neurona de contexto un 0.6 como entrada, la salida sería 0.5. Nunca sopesamos las conexiones de entrada a una neurona de contexto, pero podemos sopesar la salida de una neurona de contexto como cualquier otra conexión de red.\n",
    "\n",
    "Las neuronas de contexto nos permiten calcular una red neuronal en una sola pasada de retroalimentación. Las neuronas de contexto suelen presentarse en capas. Una capa de neuronas de contexto siempre tendrá el mismo número de neuronas de contexto que neuronas en su capa de origen, como se demuestra en la Figura 1.\n",
    "\n",
    "** Figura 1. Capas de contexto **\n",
    "![Context Layers](../images/class_10_context_layer.png \"Context Layers\")\n",
    "\n",
    "Como se puede ver en la capa anterior, dos neuronas ocultas que están etiquetadas como una oculta y dos ocultas se conectan directamente a las dos neuronas de contexto. Las líneas discontinuas en estas conexiones indican que no son conexiones ponderadas. Estas conexiones nunca son densas. Si estas conexiones fueran densas, una oculta estaría conectada tanto a la oculta como a la oculta 2. Sin embargo, la conexión directa une cada neurona oculta a su neurona de contexto correspondiente. Las dos neuronas de contexto forman conexiones densas y ponderadas con las dos neuronas ocultas. Finalmente, las dos neuronas ocultas también forman densas conexiones con las neuronas de la siguiente capa. Las dos neuronas de contexto forman dos conexiones con una sola neurona en la siguiente capa, cuatro conexiones con dos neuronas, seis conexiones con tres neuronas, y así sucesivamente.\n",
    "\n",
    "Puede combinar neuronas de contexto con las capas de entrada, ocultas y de salida de una red neuronal de muchas formas diferentes.\n",
    "Enviar comentarios\n",
    "Historial\n",
    "Guardadas\n",
    "Comunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendiendo las redes LSTM\n",
    "\n",
    "Las capas de una red neuronal  (LSTM) son un tipo de unidad recurrente que se utiliza a menudo con redes neuronales profundas.[[Cite:hochreiter1997long]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320) \n",
    "a TensorFlow, puede pensar en LSTM como un tipo de capa que puede combinar con otros tipos de capas, como densa. LSTM hace uso de dos tipos de funciones de transferencia internamente.\n",
    "\n",
    "$ \\mbox{S}(t) = \\frac{1}{1 + e^{-t}} $\n",
    "\n",
    "El segundo tipo de función de transferencia es la función de tangente hiperbólica (tanh), que puede escalar la salida del LSTM. Esta funcionalidad es similar a cómo hemos utilizado otras funciones de transferencia en este curso.\n",
    "\n",
    "Proporcionamos los gráficos para estas funciones aquí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tanh)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(1/(1+math.exp(-item)))\n",
    "    return a\n",
    "\n",
    "def f2(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(math.tanh(item))\n",
    "    return a\n",
    "\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "y1 = sigmoid(x)\n",
    "y2 = f2(x)\n",
    "\n",
    "print(\"Sigmoid\")\n",
    "plt.plot(x,y1)\n",
    "plt.show()\n",
    "\n",
    "print(\"(tanh)\")\n",
    "plt.plot(x,y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambas funciones comprimen la salida a un rango específico. Para la función sigmoide, este rango es de 0 a 1. Para la función tangente hiperbólica, este rango es de -1 a 1.\n",
    "\n",
    "LSTM mantiene un estado interno y produce una salida. El siguiente diagrama muestra una unidad LSTM en tres segmentos de tiempo: el segmento de tiempo actual (t), así como el segmento anterior (t-1) y el siguiente (t + 1), como se muestra en la Figura 2.\n",
    "\n",
    "\n",
    "**Figure 2.Capas LSTM**\n",
    "![LSTM Layers](../images/class_10_lstm1.png \"LSTM Layers\")\n",
    "\n",
    "Los valores $\\hat{y}$ son la salida; los valores ($x$) son las entradas, y los valores $c$ son el contexto.  203/5000\n",
    "Los valores de salida y contexto siempre alimentan su salida al siguiente segmento de tiempo. Los valores de contexto permiten que la red mantenga el estado entre llamadas. Figura 3 muestra los componentes internos de una capa de LSTM.\n",
    "\n",
    "**Figure 3: Interior de una capa LSTM**\n",
    "![LSTM Layers](../images/class_10_lstm2.png \"Inside the LSTM\")\n",
    "\n",
    "Una unidad LSTM consta de tres puertas:\n",
    "\n",
    "* Forget Gate ($f_t$) - Controla cuando se olvida el contexto. (MC)\n",
    "* Input Gate ($i_t$) - Controla cuando el contexto debe recordar un valor. (M+/MS)\n",
    "* Output Gate ($o_t$) - Controla cuándo se permite que el valor recordado pase de la unidad. (RM)\n",
    "\n",
    "Matemáticamente, puede pensar en el diagrama anterior como el siguiente:\n",
    "\n",
    "** Estos son valores vectoriales. **\n",
    "\n",
    "Primero, calcule el valor de la forget. Esta puerta determina si la unidad LSTM debe olvidar su memoria a corto plazo. El valor $ b $ es un bias, al igual que las neuronas de bias que vimos antes. Excepto que LSTM tiene un sesgo para cada puerta: $ b_t $, $ b_i $ y $ b_o $.\n",
    "$ f_t = S(W_f \\cdot [\\hat{y}_{t-1}, x_t] + b_f) $\n",
    "\n",
    "$ i_t = S(W_i \\cdot [\\hat{y}_{t-1},x_t] + b_i) $\n",
    "\n",
    "$ \\tilde{C}_t = \\tanh(W_C \\cdot [\\hat{y}_{t-1},x_t]+b_C) $\n",
    "\n",
    "$ C_t = f_t \\cdot C_{t-1}+i_t \\cdot \\tilde{C}_t $\n",
    "\n",
    "$ o_t = S(W_o \\cdot [\\hat{y}_{t-1},x_t] + b_o ) $\n",
    "\n",
    "$ \\hat{y}_t = o_t \\cdot \\tanh(C_t) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo Simple en  TensorFlow \n",
    "\n",
    "El siguiente código crea la red LSTM, que es un ejemplo de un RNN para clasificación. El siguiente código se entrena en un conjunto de datos (x) con un tamaño máximo de secuencia de 6 (columnas) y seis elementos de entrenamiento (filas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "Construir el modelo...\n",
      "Train...\n",
      "Train on 6 samples\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 11s 2s/sample - loss: 0.6861 - accuracy: 0.7083\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.6784 - accuracy: 0.7500\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.6787 - accuracy: 0.7500\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.6774 - accuracy: 0.7500\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 8ms/sample - loss: 0.6680 - accuracy: 0.7500\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.6490 - accuracy: 0.7500\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 7ms/sample - loss: 0.6593 - accuracy: 0.7500\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 8ms/sample - loss: 0.6293 - accuracy: 0.7500\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 8ms/sample - loss: 0.6244 - accuracy: 0.7500\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 10ms/sample - loss: 0.6120 - accuracy: 0.7500\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.6039 - accuracy: 0.7500\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.6161 - accuracy: 0.7500\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 8ms/sample - loss: 0.5893 - accuracy: 0.7917\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.6160 - accuracy: 0.7917\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.5681 - accuracy: 0.7917\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.5822 - accuracy: 0.7500\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.5508 - accuracy: 0.7917\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.5416 - accuracy: 0.7917\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.6016 - accuracy: 0.7917\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.5659 - accuracy: 0.7500\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.5712 - accuracy: 0.7500\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 9ms/sample - loss: 0.5299 - accuracy: 0.7500\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.5248 - accuracy: 0.7500\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 7ms/sample - loss: 0.5592 - accuracy: 0.7500\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.5223 - accuracy: 0.7500\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.5280 - accuracy: 0.7500\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4843 - accuracy: 0.7500\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4774 - accuracy: 0.7500\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.5199 - accuracy: 0.7500\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.5239 - accuracy: 0.7500\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4619 - accuracy: 0.7500\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.4751 - accuracy: 0.7500\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4742 - accuracy: 0.7500\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4546 - accuracy: 0.7500\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.4589 - accuracy: 0.7500\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4458 - accuracy: 0.7500\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4311 - accuracy: 0.7500\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4326 - accuracy: 0.7500\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4598 - accuracy: 0.7500\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4603 - accuracy: 0.7500\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4470 - accuracy: 0.7500\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.4154 - accuracy: 0.7917\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 9ms/sample - loss: 0.4438 - accuracy: 0.7917\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4232 - accuracy: 0.7917\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4296 - accuracy: 0.7500\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4500 - accuracy: 0.6667\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.3755 - accuracy: 0.8333\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4102 - accuracy: 0.7083\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3734 - accuracy: 0.7917\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4299 - accuracy: 0.7500\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3721 - accuracy: 0.7917\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3717 - accuracy: 0.7917\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4093 - accuracy: 0.7500\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3794 - accuracy: 0.7500\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3982 - accuracy: 0.7500\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4043 - accuracy: 0.8333\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3579 - accuracy: 0.8750\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3420 - accuracy: 0.9167\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.5257 - accuracy: 0.7083\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3612 - accuracy: 0.8333\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.4545 - accuracy: 0.7917\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4001 - accuracy: 0.7917\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3253 - accuracy: 0.9167\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4171 - accuracy: 0.8750\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3814 - accuracy: 0.8750\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4517 - accuracy: 0.8750\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4552 - accuracy: 0.8333\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3351 - accuracy: 0.8333\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3337 - accuracy: 0.7917\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3516 - accuracy: 0.8333\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3007 - accuracy: 0.9167\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 7ms/sample - loss: 0.3941 - accuracy: 0.8333\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3048 - accuracy: 0.8333\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4071 - accuracy: 0.7917\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3889 - accuracy: 0.7917\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3538 - accuracy: 0.7917\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4357 - accuracy: 0.7917\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3927 - accuracy: 0.8333\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.2596 - accuracy: 0.9583\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3669 - accuracy: 0.8333\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3971 - accuracy: 0.7083\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4197 - accuracy: 0.7083\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2740 - accuracy: 0.8750\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4744 - accuracy: 0.7083\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3929 - accuracy: 0.8333\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3386 - accuracy: 0.8750\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2831 - accuracy: 0.9167\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3056 - accuracy: 0.9167\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4042 - accuracy: 0.8333\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3697 - accuracy: 0.8333\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.2676 - accuracy: 0.8750\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2722 - accuracy: 0.9167\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4933 - accuracy: 0.7500\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.2808 - accuracy: 0.9167\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.5628 - accuracy: 0.6667\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 7ms/sample - loss: 0.2712 - accuracy: 0.9583\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.2626 - accuracy: 0.8750\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 9ms/sample - loss: 0.4607 - accuracy: 0.7500\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 7ms/sample - loss: 0.3418 - accuracy: 0.8333\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 7ms/sample - loss: 0.4968 - accuracy: 0.7083\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3718 - accuracy: 0.8333\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.4668 - accuracy: 0.7917\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.2537 - accuracy: 0.9583\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 8ms/sample - loss: 0.2670 - accuracy: 0.9583\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3103 - accuracy: 0.9167\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.2718 - accuracy: 0.9583\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2812 - accuracy: 0.8750\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4145 - accuracy: 0.8333\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.2561 - accuracy: 0.9167\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3031 - accuracy: 0.8750\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4746 - accuracy: 0.7500\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.2635 - accuracy: 0.9167\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 7ms/sample - loss: 0.2865 - accuracy: 0.8750\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3687 - accuracy: 0.7917\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3577 - accuracy: 0.7917\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2805 - accuracy: 0.8333\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2806 - accuracy: 0.8750\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3554 - accuracy: 0.8333\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3657 - accuracy: 0.8333\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2318 - accuracy: 0.9583\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2640 - accuracy: 0.9167\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3809 - accuracy: 0.8750\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3752 - accuracy: 0.8750\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3596 - accuracy: 0.8333\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3493 - accuracy: 0.8750\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3587 - accuracy: 0.7917\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.5009 - accuracy: 0.7500\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3453 - accuracy: 0.8750\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 3ms/sample - loss: 0.3587 - accuracy: 0.8333\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 3ms/sample - loss: 0.2274 - accuracy: 0.9167\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 3ms/sample - loss: 0.2584 - accuracy: 0.9167\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3007 - accuracy: 0.8750\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2749 - accuracy: 0.8750\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3375 - accuracy: 0.8333\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2184 - accuracy: 0.9583\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.2582 - accuracy: 0.8750\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3763 - accuracy: 0.7500\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.3524 - accuracy: 0.7917\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2469 - accuracy: 0.9167\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2073 - accuracy: 0.9583\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3323 - accuracy: 0.8750\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 3ms/sample - loss: 0.3119 - accuracy: 0.8750\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2944 - accuracy: 0.8750\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.1965 - accuracy: 0.9583\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.1808 - accuracy: 0.9583\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1743 - accuracy: 0.9583\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3262 - accuracy: 0.8750\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2466 - accuracy: 0.9167\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1757 - accuracy: 0.9583\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4522 - accuracy: 0.7917\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1886 - accuracy: 0.9167\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4696 - accuracy: 0.7917\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.2110 - accuracy: 0.9583\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3950 - accuracy: 0.7917\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3659 - accuracy: 0.8333\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 7ms/sample - loss: 0.1760 - accuracy: 0.9583\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3949 - accuracy: 0.7917\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2241 - accuracy: 0.9583\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 3ms/sample - loss: 0.2967 - accuracy: 0.8333\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4558 - accuracy: 0.7500\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.2963 - accuracy: 0.8750\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1726 - accuracy: 0.9583\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.1986 - accuracy: 0.9583\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1626 - accuracy: 0.9583\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1703 - accuracy: 0.9583\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.1913 - accuracy: 0.9167\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 6ms/sample - loss: 0.1810 - accuracy: 0.9583\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3716 - accuracy: 0.7917\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1605 - accuracy: 0.9583\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1599 - accuracy: 0.9583\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 3ms/sample - loss: 0.3129 - accuracy: 0.8750\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 3ms/sample - loss: 0.2802 - accuracy: 0.8750\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3004 - accuracy: 0.8750\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.1462 - accuracy: 0.9583\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3259 - accuracy: 0.8333\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3371 - accuracy: 0.8333\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.3915 - accuracy: 0.8333\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.1609 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1391 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1356 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 3ms/sample - loss: 0.1904 - accuracy: 0.9583\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.2921 - accuracy: 0.8750\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2872 - accuracy: 0.9167\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.2551 - accuracy: 0.8750\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.2115 - accuracy: 0.9167\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2699 - accuracy: 0.9167\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2246 - accuracy: 0.9167\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1166 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1416 - accuracy: 0.9583\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.3774 - accuracy: 0.8333\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1567 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.4195 - accuracy: 0.7500\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2531 - accuracy: 0.9167\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 3ms/sample - loss: 0.2172 - accuracy: 0.9167\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.1324 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.2584 - accuracy: 0.9167\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1167 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 3ms/sample - loss: 0.2321 - accuracy: 0.9167\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1286 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 4ms/sample - loss: 0.1336 - accuracy: 1.0000\n",
      "Predicted classes: {} [1 2 3 2 2 1]\n",
      "Expected classes: {} [1 2 3 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import numpy as np\n",
    "\n",
    "max_features = 4 # 0,1,2,3 (total of 4)\n",
    "x = [\n",
    "    [[0],[1],[1],[0],[0],[0]],\n",
    "    [[0],[0],[0],[2],[2],[0]],\n",
    "    [[0],[0],[0],[0],[3],[3]],\n",
    "    [[0],[2],[2],[0],[0],[0]],\n",
    "    [[0],[0],[3],[3],[0],[0]],\n",
    "    [[0],[0],[0],[0],[1],[1]]\n",
    "]\n",
    "x = np.array(x,dtype=np.float32)\n",
    "y = np.array([1,2,3,2,3,1],dtype=np.int32)\n",
    "\n",
    "# Convert y2 to dummy variables\n",
    "y2 = np.zeros((y.shape[0], max_features),dtype=np.float32)\n",
    "y2[np.arange(y.shape[0]), y] = 1.0\n",
    "print(y2)\n",
    "\n",
    "print('Construir el modelo...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, \\\n",
    "               input_shape=(None, 1)))\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x,y2,epochs=200)\n",
    "pred = model.predict(x)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "print(\"Predicted classes: {}\",predict_classes)\n",
    "print(\"Expected classes: {}\",predict_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def runit(model, inp):\n",
    "    inp = np.array(inp,dtype=np.float32)\n",
    "    pred = model.predict(inp)\n",
    "    return np.argmax(pred[0])\n",
    "\n",
    "print( runit( model, [[[0],[0],[0],[0],[0],[1]]] ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15193452248967720763\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1401988300\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15961410948227089699\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos de los ciclos solares\n",
    "En esta sección, vemos un ejemplo de regresión RNN para predecir manchas solares. Puede encontrar los archivos de datos necesarios para este ejemplo en la siguiente ubicación.\n",
    "\n",
    "* [Download Daily Sunspots](http://www.sidc.be/silso/INFO/sndtotcsv.php) - 1/1/1818 to now.\n",
    "\n",
    "El siguiente código carga el archivo de manchas solares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio :\n",
      "   year  month  day  dec_year  sn_value  sn_error  obs_num\n",
      "0  1818      1    1  1818.001        -1       NaN        0\n",
      "1  1818      1    2  1818.004        -1       NaN        0\n",
      "2  1818      1    3  1818.007        -1       NaN        0\n",
      "3  1818      1    4  1818.010        -1       NaN        0\n",
      "4  1818      1    5  1818.012        -1       NaN        0\n",
      "5  1818      1    6  1818.015        -1       NaN        0\n",
      "6  1818      1    7  1818.018        -1       NaN        0\n",
      "7  1818      1    8  1818.021        65      10.2        1\n",
      "8  1818      1    9  1818.023        -1       NaN        0\n",
      "9  1818      1   10  1818.026        -1       NaN        0\n",
      "Fin:\n",
      "       year  month  day  dec_year  sn_value  sn_error  obs_num\n",
      "74074  2020     10   22  2020.807        11       1.0       34\n",
      "74075  2020     10   23  2020.810        22       0.9       16\n",
      "74076  2020     10   24  2020.813        21       0.8       17\n",
      "74077  2020     10   25  2020.816        22       1.6        7\n",
      "74078  2020     10   26  2020.818        19       1.6       23\n",
      "74079  2020     10   27  2020.821        44       3.9       11\n",
      "74080  2020     10   28  2020.824        28       2.0       11\n",
      "74081  2020     10   29  2020.827        37       3.1       18\n",
      "74082  2020     10   30  2020.829        31       2.7       15\n",
      "74083  2020     10   31  2020.832        26       1.3       22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "PATH = \"../data/\"\n",
    "    \n",
    "filename = os.path.join(PATH,\"SN_d_tot_V2.0.csv\")    \n",
    "names = ['year', 'month', 'day', 'dec_year', 'sn_value' , \n",
    "         'sn_error', 'obs_num']\n",
    "df = pd.read_csv(filename,sep=';',header=None,names=names,\n",
    "                 na_values=['-1'], index_col=False)\n",
    "\n",
    "print(\"Inicio :\")\n",
    "print(df[0:10])\n",
    "\n",
    "print(\"Fin:\")\n",
    "print(df[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ver, faltan bastantes datos cerca del final del archivo. Queremos encontrar el índice inicial donde los datos faltantes ya no ocurren. Esta técnica es algo descuidada; sería mejor encontrar un uso para los datos entre los valores perdidos. Sin embargo, el objetivo de este ejemplo es mostrar cómo usar LSTM con una serie de tiempo algo simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "start_id = max(df[df['obs_num'] == 0].index.tolist())+1  # Encontrar el primer cero\n",
    "print(start_id)\n",
    "df = df[start_id:] # Remover las observaciones faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El conjunto de entrenamiento 55160 observaciones.\n",
      "El conjunto de validación 7610 observaciones.\n"
     ]
    }
   ],
   "source": [
    "df['sn_value'] = df['sn_value'].astype(float)\n",
    "df_train = df[df['year']<2000]\n",
    "df_test = df[df['year']>=2000]\n",
    "\n",
    "spots_train = df_train['sn_value'].tolist()\n",
    "spots_test = df_test['sn_value'].tolist()\n",
    "\n",
    "print(\"El conjunto de entrenamiento {} observaciones.\".format(len(spots_train)))\n",
    "print(\"El conjunto de validación {} observaciones.\".format(len(spots_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del conjunto de entrenamiento: (55150, 10, 1)\n",
      "Forma del conjunto de validación: (7600, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_sequences(seq_size, obs):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(obs)-SEQUENCE_SIZE):\n",
    "        #print(i)\n",
    "        window = obs[i:(i+SEQUENCE_SIZE)]\n",
    "        after_window = obs[i+SEQUENCE_SIZE]\n",
    "        window = [[x] for x in window]\n",
    "        #print(\"{} - {}\".format(window,after_window))\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "        \n",
    "    return np.array(x),np.array(y)\n",
    "    \n",
    "    \n",
    "SEQUENCE_SIZE = 10\n",
    "x_train,y_train = to_sequences(SEQUENCE_SIZE,spots_train)\n",
    "x_test,y_test = to_sequences(SEQUENCE_SIZE,spots_test)\n",
    "\n",
    "print(\"Forma del conjunto de entrenamiento: {}\".format(x_train.shape))\n",
    "print(\"Forma del conjunto de validación: {}\".format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[353.],\n",
       "        [240.],\n",
       "        [275.],\n",
       "        ...,\n",
       "        [340.],\n",
       "        [238.],\n",
       "        [287.]],\n",
       "\n",
       "       [[240.],\n",
       "        [275.],\n",
       "        [352.],\n",
       "        ...,\n",
       "        [238.],\n",
       "        [287.],\n",
       "        [294.]],\n",
       "\n",
       "       [[275.],\n",
       "        [352.],\n",
       "        [268.],\n",
       "        ...,\n",
       "        [287.],\n",
       "        [294.],\n",
       "        [342.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[138.],\n",
       "        [141.],\n",
       "        [128.],\n",
       "        ...,\n",
       "        [116.],\n",
       "        [ 95.],\n",
       "        [ 85.]],\n",
       "\n",
       "       [[141.],\n",
       "        [128.],\n",
       "        [130.],\n",
       "        ...,\n",
       "        [ 95.],\n",
       "        [ 85.],\n",
       "        [103.]],\n",
       "\n",
       "       [[128.],\n",
       "        [130.],\n",
       "        [123.],\n",
       "        ...,\n",
       "        [ 85.],\n",
       "        [103.],\n",
       "        [ 66.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construir el modelo...\n",
      "Train...\n",
      "Train on 55150 samples, validate on 7600 samples\n",
      "Epoch 1/1000\n",
      "55150/55150 - 30s - loss: 1326.7264 - val_loss: 187.1982\n",
      "Epoch 2/1000\n",
      "55150/55150 - 15s - loss: 514.4894 - val_loss: 197.9369\n",
      "Epoch 3/1000\n",
      "55150/55150 - 16s - loss: 511.1427 - val_loss: 183.7227\n",
      "Epoch 4/1000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "print('Construir el modelo...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, dropout=0.0, recurrent_dropout=0.0,input_shape=(None, 1)))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, \n",
    "                        verbose=1, mode='auto', restore_best_weights=True)\n",
    "print('Entrenar...')\n",
    "\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
    "          callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar el RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec LSTM\n",
    "\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
